# -*- coding: utf-8 -*-
"""movie_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yX4qfpoLUMI4BNAeE8a4SlJ73GkbRcEC
"""

p = [1000, 1002, 1025, 1039, 1046]  #你要預測的人喜歡的電影
person_n=100  #你要用多人人去算相似度，越多越準但時間越長



# code for loading the format for the notebook
import os

# path : store the current path to convert back to it later
path = os.getcwd()
#os.chdir(os.path.join('..', 'notebook_format'))
#from formats import load_style
#load_style(css_style = 'custom2.css', plot_style = False)

#os.chdir(path)

# 1. magic to print version
# 2. magic so that the notebook will reload external python modules


import sys
import numpy as np
import pandas as pd
from math import ceil
from tqdm import trange
from subprocess import call
from itertools import islice
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import normalize
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix, dok_matrix
import joblib
import random
import pandas as pd
from scipy import sparse
from collections import Counter

class BPR:
    
    def __init__(self, learning_rate = 0.01, n_factors = 15, n_iters = 10, 
                 batch_size = 1000, reg = 0.01, seed = 1234, verbose = True):
        self.reg = reg
        self.seed = seed
        self.verbose = verbose
        self.n_iters = n_iters
        self.n_factors = n_factors
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # to avoid re-computation at predict
        self._prediction = None
        
    def fit(self, ratings):
        
        indptr = ratings.indptr
        indices = ratings.indices
        n_users, n_items = ratings.shape
        #print(ratings.shape)
        
        # ensure batch size makes sense, since the algorithm involves
        # for each step randomly sample a user, thus the batch size
        # should be smaller than the total number of users or else
        # we would be sampling the user with replacement
        batch_size = self.batch_size
        if n_users < batch_size:
            batch_size = n_users
            sys.stderr.write('WARNING: Batch size is greater than number of users,'
                             'switching to a batch size of {}\n'.format(n_users))

        batch_iters = n_users // batch_size
        
        # initialize random weights
        rstate = np.random.RandomState(self.seed)
        #print(n_users)
        #print(self.n_factors)
        self.user_factors = rstate.normal(size = (n_users, self.n_factors))
        self.item_factors = rstate.normal(size = (n_items, self.n_factors))
        #print(self.user_factors)
        #print(self.item_factors)
        
        
        # progress bar for training iteration if verbose is turned on
        loop = range(self.n_iters)
        if self.verbose:
            loop = trange(self.n_iters, desc = self.__class__.__name__)
        
        for _ in loop:
            for _ in range(batch_iters):
                sampled = self._sample(n_users, n_items, indices, indptr)###
                #print(sampled[0])
                sampled_users, sampled_pos_items, sampled_neg_items = sampled
                self._update(sampled_users, sampled_pos_items, sampled_neg_items)

        return self
    
    def _sample(self, n_users, n_items, indices, indptr):
        """sample batches of random triplets u, i, j"""
        sampled_pos_items = np.zeros(self.batch_size, dtype = np.int)
        sampled_neg_items = np.zeros(self.batch_size, dtype = np.int)
        sampled_users = np.random.choice(
            n_users, size = self.batch_size, replace = False)
        i=0
        for idx, user in enumerate(sampled_users):
            pos_items = indices[indptr[user]:indptr[user + 1]]
            #print(pos_items)###############################
            ##print(i)
            i+=1
            #print(pos_items[18])
            #print(pos_items[19])
            """
             pos_item = np.random.choice(pos_items)##
             neg_item = np.random.choice(n_items)
             while neg_item in pos_items:
                 neg_item = np.random.choice(n_items)

             sampled_pos_items[idx] = pos_item
             sampled_neg_items[idx] = neg_item
"""
            if len(pos_items) !=0:
                pos_item = np.random.choice(pos_items)##
                neg_item = np.random.choice(n_items)
                while neg_item in pos_items:
                    neg_item = np.random.choice(n_items)

                sampled_pos_items[idx] = pos_item
                sampled_neg_items[idx] = neg_item
                
        #print(pos_item,neg_item)
        #print(pos_items)
        
        return sampled_users, sampled_pos_items, sampled_neg_items
                
    def _update(self, u, i, j):
        
        user_u = self.user_factors[u]
        item_i = self.item_factors[i]
        item_j = self.item_factors[j]
        
        # decompose the estimator, compute the difference between
        # the score of the positive items and negative items; a
        # naive implementation might look like the following:
        # r_ui = np.diag(user_u.dot(item_i.T))
        # r_uj = np.diag(user_u.dot(item_j.T))
        # r_uij = r_ui - r_uj
        
        # however, we can do better, so
        # for batch dot product, instead of doing the dot product
        # then only extract the diagonal element (which is the value
        # of that current batch), we perform a hadamard product, 
        # i.e. matrix element-wise product then do a sum along the column will
        # be more efficient since it's less operations
        # http://people.revoledu.com/kardi/tutorial/LinearAlgebra/HadamardProduct.html
        # r_ui = np.sum(user_u * item_i, axis = 1)
        #
        # then we can achieve another speedup by doing the difference
        # on the positive and negative item up front instead of computing
        # r_ui and r_uj separately, these two idea will speed up the operations
        # from 1:14 down to 0.36
        r_uij = np.sum(user_u * (item_i - item_j), axis = 1)
        sigmoid = np.exp(-r_uij) / (1.0 + np.exp(-r_uij))
        
        # repeat the 1 dimension sigmoid n_factors times so
        # the dimension will match when doing the update
        sigmoid_tiled = np.tile(sigmoid, (self.n_factors, 1)).T

        # update using gradient descent
        grad_u = sigmoid_tiled * (item_j - item_i) + self.reg * user_u
        grad_i = sigmoid_tiled * -user_u + self.reg * item_i
        grad_j = sigmoid_tiled * user_u + self.reg * item_j
        self.user_factors[u] -= self.learning_rate * grad_u
        self.item_factors[i] -= self.learning_rate * grad_i
        self.item_factors[j] -= self.learning_rate * grad_j
        return self

    def predict(self):
       
        if self._prediction is None:
            self._prediction = self.user_factors.dot(self.item_factors.T)

        return self._prediction

    def _predict_user(self, user):
        
        user_pred = self.user_factors[user].dot(self.item_factors.T)
        return user_pred

    def recommend(self, ratings, N = 5):
        
        n_users = ratings.shape[0]
        recommendation = np.zeros((n_users, N), dtype = np.uint32)
        for user in range(n_users):
            top_n = self._recommend_user(ratings, user, N)
            recommendation[user] = top_n

        return recommendation

    def _recommend_user(self, ratings, user, N):
        """the top-N ranked items for a given user"""
        scores = self._predict_user(user)

        # compute the top N items, removing the items that the user already liked
        # from the result and ensure that we don't get out of bounds error when 
        # we ask for more recommendations than that are available
        liked = set(ratings[user].indices)
        count = N + len(liked)
        if count < scores.shape[0]:

            # when trying to obtain the top-N indices from the score,
            # using argpartition to retrieve the top-N indices in 
            # unsorted order and then sort them will be faster than doing
            # straight up argort on the entire score
            # http://stackoverflow.com/questions/42184499/cannot-understand-numpy-argpartition-output
            ids = np.argpartition(scores, -count)[-count:]
            best_ids = np.argsort(scores[ids])[::-1]
            best = ids[best_ids]
        else:
            best = np.argsort(scores)[::-1]

        top_n = list(islice((rec for rec in best if rec not in liked), N))
        return top_n
    
    def get_similar_items(self, N = 5, item_ids = None):
        
        # cosine distance is proportional to normalized euclidean distance,
        # thus we normalize the item vectors and use euclidean metric so
        # we can use the more efficient kd-tree for nearest neighbor search;
        # also the item will always to nearest to itself, so we add 1 to 
        # get an additional nearest item and remove itself at the end
        normed_factors = normalize(self.item_factors)
        knn = NearestNeighbors(n_neighbors = N + 1, metric = 'euclidean')
        knn.fit(normed_factors)

        # returns a distance, index tuple,
        # we don't actually need the distance
        if item_ids is not None:
            normed_factors = normed_factors[item_ids]

        _, items = knn.kneighbors(normed_factors)
        similar_items = items[:, 1:].astype(np.uint32)
        return similar_items





bpr = joblib.load('model_weight.sav')

#file_dir = 'ml-100k'C:\Users\Sophie\Desktop\ml-1m

#file_dir = 'C:/Users/Sophie/Desktop/ml-100k'
#file_path = os.path.join(file_dir, 'ratings.dat')#u.data


# if not os.path.isdir(file_dir):
#     call(['curl', '-O', 'http://files.grouplens.org/datasets/movielens/' + file_dir + '.zip'])
#     call(['unzip', file_dir + '.zip'])

# we will not be using the timestamp column
names = ['user_id', 'item_id', 'rating', 'timestamp']
#df = pd.read_csv(file_path, sep = '\t', names = names)#\t
rating_df = pd.read_csv('ml-latest/ratings.csv',sep=',',names = names,skiprows=1)#
# df = pd.read_table('ml-latest/ratings.csv', sep = '::', names = names)#\t
# print('data dimension: \n', df.shape)
rating_df.head()
# print(df)

X = sparse.load_npz("array.npz")



#user id & movie id from 1
#add 1= moive id
def recommend_movie(p):
    same_user=similarity_user(p)
    #print(same_user)
    #0-user
    user_like5=bpr.recommend(X[0:person_n], N = 5)[same_user]
    #print(user_like5)
    #add 1= moive id
    user_like5=user_like5+1
    #print(user_like5)
    return(user_like5)

#per user like movie >3
#input form 1 to len
user_like=[]

array=X[0:person_n].toarray()
for i in range(0,len(array)):
    per=[]
    user_id=i+1
    per.append(user_id)
    #user_like.append(per)
    per_like=[]
    for j in range(0,len(array[i])):
        
    #print("A")
    #print(i)
        
        if array[i][j]==1:
            
            B=j+1
            per_like.append(B)
    per.append(per_like)
    user_like.append(per)        
            #per.append(B)
#print(user_like)
#print(len(user_like))

# user from 0

def similarity_user(p):
    similarity_array=[]
    for i in range(0,len(array)):
        q = user_like[i][1]
        #print(A)
        similarity_per=pearson(p,q)
        similarity_array.append(similarity_per)

    find = max(similarity_array)
    max_array=[i for i,v in enumerate(similarity_array) if v==find]

    choice=random.randint(0,len(max_array)-1)
    same_user=max_array[choice] 

    #print(choice)
    #print(same_user)
    return same_user



def pearson(p,q):
    
    same_or_not=[l for l in p if l in q]
    if len(same_or_not)==0:
        r=0
    else:
#只計算兩者共同有的
        same = 0
        for i in p:
            if i in q:
                same +=1

        n = same
        #分別求p，q的和
        sumx = sum([p[i] for i in range(n)])
        sumy = sum([q[i] for i in range(n)])
        #分別求出p，q的平方和
        sumxsq = sum([p[i]**2 for i in range(n)])
        sumysq = sum([q[i]**2 for i in range(n)])
        #求出p，q的乘積和
        sumxy = sum([p[i]*q[i] for i in range(n)])
        # print sumxy
        #求出pearson相關係數
        up = sumxy - sumx*sumy/n
        down = ((sumxsq - pow(sumxsq,2)/n)*(sumysq - pow(sumysq,2)/n))**.5
        #若down為零則不能計算，return 0
        if down == 0 :return 0
        r = up/down
    return r

recommend_movie(p)
recommend_movie_id=recommend_movie(p)
print(recommend_movie_id)



















